#run_analysis.py
import sys
import os
import pandas as pd
import pickle
import time
from datetime import datetime
from tqdm import tqdm 
import numpy as np

# =========================================================
# 1. ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
# =========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# ì™¸ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
from Region_module.processor import RegionProcessor
from Region_module import config as region_config
from Region_module.sampler import PointSampler
from Path_module.main import run_path_analysis 
from Path_module import config as path_config
from integrated_viz import IntegratedVisualizer
from lof_processor import calculate_lof_scores

# ğŸ’¡ ë¶„ì„ ìƒìˆ˜
LOF_NORMAL_THRESHOLD = 1.2
CACHE_FILE = "analysis_cache.pkl"

def analyze_path_data(file_name, original_name):
    """Path_module ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    path_config.FILE_NAME_PATHS = file_name
    path_results = run_path_analysis() 
    path_config.FILE_NAME_PATHS = original_name
    return path_results

def main():
    total_start_time = time.time()
    original_file_name = path_config.FILE_NAME_PATHS 
    
    # ------------------------------------------------------------------
    # 1. ìºì‹œ í™•ì¸ ë° ë°ì´í„° ë¡œë“œ
    # ------------------------------------------------------------------
    cache_path = os.path.join(current_dir, CACHE_FILE)
    use_cache = False
    
    cached_data = {}
    if os.path.exists(cache_path):
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] ğŸš€ ìºì‹œ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        try:
            with open(cache_path, 'rb') as f:
                cached_data = pickle.load(f)
            use_cache = True
            print(" -> ìºì‹œ ë¡œë“œ ì„±ê³µ.")
        except Exception as e:
            print(f" -> ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}. ì „ì²´ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # ------------------------------------------------------------------
    # 2. ë°ì´í„° ì¤€ë¹„ (Original Path & Regions)
    # ------------------------------------------------------------------
    
    # A. ì›ë³¸ ê²½ë¡œ ë¶„ì„
    if use_cache and 'original_path_results' in cached_data:
        original_path_results = cached_data['original_path_results']
    else:
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] 1ï¸âƒ£-A. ê¸°ì¡´ ê²½ë¡œ ë¶„ì„ ì¤‘ (Full Run)...")
        original_path_results = analyze_path_data(original_file_name, original_file_name)
    
    center_lat, center_lon = original_path_results['center_coords']

    # B. ë¦¬ì „ ë¶„ì„ ë° ìƒ˜í”Œë§
    if use_cache and 'region_data' in cached_data:
        poly_df = cached_data['region_data']['poly_df']
        region_sample_df = cached_data['region_data']['region_sample_df']
        raw_df = cached_data['region_data']['raw_df']
    else:
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] 2ï¸âƒ£ ë¦¬ì „ ë° í´ë¦¬ê³¤ ìƒì„± ì¤‘...")
        raw_df = pd.read_csv(region_config.STAY_POINT_FILE)
        raw_df.rename(columns={'latitude': 'centroid_lat', 'longitude': 'centroid_lon'}, inplace=True, errors='ignore')
        
        proc = RegionProcessor(raw_df.copy())
        proc.run_dbscan()
        poly_df = proc.create_polygons() 
        raw_df = proc.df # ì—…ë°ì´íŠ¸ëœ ì •ë³´(is_hull ë“±) ì €ì¥
        
        sampler = PointSampler()
        region_sample_df = sampler.sample_from_polygons(poly_df)

    # ìºì‹œ ì €ì¥
    if not use_cache:
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ’¾ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤...")
        cache_data_to_save = {
            'original_path_results': original_path_results,
            'region_data': {
                'poly_df': poly_df,
                'region_sample_df': region_sample_df,
                'raw_df': raw_df
            }
        }
        with open(cache_path, 'wb') as f:
            pickle.dump(cache_data_to_save, f)

    # ------------------------------------------------------------------
    # 3. í…ŒìŠ¤íŠ¸ ê²½ë¡œ ë¶„ì„ (Test Path)
    # ------------------------------------------------------------------
    test_file_name = "LOF_score_test.csv"
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] 1ï¸âƒ£-B. í…ŒìŠ¤íŠ¸ ê²½ë¡œ(Analysis Target) ë¶„ì„ ì¤‘...")
    test_path_results = analyze_path_data(test_file_name, original_file_name) 

    # ------------------------------------------------------------------
    # 4. LOF ìŠ¤ì½”ì–´ ê³„ì‚°
    # ------------------------------------------------------------------
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] 3ï¸âƒ£ LOF ë°°íšŒ ì§€ìˆ˜ ë¶„ì„ ì¤‘...")
    LOF_K_NEIGHBORS = getattr(path_config, 'LOF_K_NEIGHBORS', 60) 
    
    all_path_interp_points = (
        original_path_results.get('interp_points', []) + 
        test_path_results.get('interp_points', [])
    )
    
    region_coords = region_sample_df[['latitude', 'longitude']].values.tolist()
    safe_path_points = original_path_results.get('interp_points', [])
    combined_safe_coords = region_coords + safe_path_points
    safe_df = pd.DataFrame(combined_safe_coords, columns=['latitude', 'longitude'])

    lof_scores = calculate_lof_scores(
        path_points=all_path_interp_points, 
        region_points_df=safe_df, 
        k_neighbors=LOF_K_NEIGHBORS
    )

    # ------------------------------------------------------------------
    # 5. í†µí•© ì‹œê°í™” (ì´ë¯¸ì§€ 2ë²ˆì²˜ëŸ¼ ì„ ì´ ë‚˜ì˜¤ê²Œ í•˜ëŠ” í•µì‹¬ ì„¹ì…˜)
    # ------------------------------------------------------------------
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] í†µí•© ì§€ë„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    
    viz = IntegratedVisualizer(center_lat, center_lon)

    # 1. GPS ì›ë³¸ ì  (Stay Points)
    if not raw_df.empty:
        viz.add_raw_points(raw_df)

    # 2. ë¨¸ë¬´ë¦„ ì§€ì—­ (Regions)
    if not poly_df.empty:
        viz.add_regions_from_module(poly_df)

    # ğŸš¨ [í•µì‹¬] 3. ì›ë³¸ ë§¤ì¹­ ê²½ë¡œ (OSMnx ë…¸ë“œë¥¼ ë”°ë¼ê°€ëŠ” íŒŒë€ìƒ‰ ì„ )
    # 3-A. ê¸°ì¡´ì˜ ì•ˆì „ ê²½ë¡œ ë§¤ì¹­ ì„ 
    if 'final_grouped_lines' in original_path_results:
        viz.add_final_path_chunks(
            original_path_results['final_grouped_lines'], 
            layer_name="Original Path (OSMnx Merged)"
        )

    # 4. Test Path Matching (í…ŒìŠ¤íŠ¸ ê²½ë¡œì˜ ë¨¸ì§€ëœ ì„ )
    # ì´ë¯¸ì§€ 2ë²ˆì²˜ëŸ¼ ë…¸ë“œ ë”°ë¼ ì´ì–´ì§„ ì„ ì„ ë³´ê³  ì‹¶ë‹¤ë©´ ì´ ë°ì´í„°ê°€ í•µì‹¬ì…ë‹ˆë‹¤.
    if 'final_grouped_lines' in test_path_results:
        viz.add_final_path_chunks(
            test_path_results['final_grouped_lines'], 
            layer_name="Test Path (OSMnx Merged)"
        )

    # 4. LOF ê²°ê³¼ ì  ì‹œê°í™”
    viz.add_sample_points(
        all_path_interp_points, 
        layer_name="All Path LOF Scores", 
        default_show=True,
        lof_scores=lof_scores
    )
    
    # 5. ë°°ê²½ ìƒ˜í”Œë§ ë°ì´í„°
    if not region_sample_df.empty:
        viz.add_sample_points(
            region_sample_df[['latitude', 'longitude']].values.tolist(), 
            layer_name="Region Area Samples (Background)",
            default_show=False 
        )
    
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_path = os.path.join(current_dir, f"Integrated_Map_{ts}.html")
    viz.save(save_path)

    total_elapsed = time.time() - total_start_time
    print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {total_elapsed:.2f}ì´ˆ")

if __name__ == "__main__":
    main()